{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 1: Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and Pydantic Models\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional, Dict\n",
    "import pandas as pd\n",
    "import fitz  # PyMuPDF\n",
    "import ollama\n",
    "import json\n",
    "import os\n",
    "from langchain_ollama import OllamaEmbeddings, ChatOllama\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Pydantic model for command extraction\n",
    "class QueryCommand(BaseModel):\n",
    "    command: str = Field(..., description=\"Command type (e.g., summarize, answer_question, search, compare_files)\")\n",
    "    params: Dict = Field(default_factory=dict, description=\"Parameters like keyword, file_name, file_type\")\n",
    "\n",
    "# Pydantic model for response formatting\n",
    "class ChatbotResponse(BaseModel):\n",
    "    answer: str = Field(..., description=\"Concise response to the query, under 100 words\", min_length=1, max_length=100)\n",
    "    sources: List[str] = Field(default_factory=list, description=\"List of file names referenced in the answer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 2: Load docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document Loading Module\n",
    "def load_document(file_path: str) -> tuple[str, List[Dict]]:\n",
    "    \"\"\"Parse a document (Excel, PDF, TXT) into chunks with metadata.\"\"\"\n",
    "    chunks = []\n",
    "    file_name = os.path.basename(file_path)\n",
    "    file_ext = os.path.splitext(file_name)[1].lower()\n",
    "    \n",
    "    if file_ext in ['.xlsx', '.xls']:\n",
    "        sheets = pd.read_excel(file_path, sheet_name=None)\n",
    "        for sheet_name, df in sheets.items():\n",
    "            for i, row in df.iterrows():\n",
    "                chunks.append({\"text\": f\"Sheet: {sheet_name}, Row {i}: {row.to_dict()}\", \n",
    "                              \"metadata\": {\"file_name\": file_name, \"file_type\": \"excel\", \"sheet\": sheet_name}})\n",
    "    elif file_ext == '.pdf':\n",
    "        doc = fitz.open(file_path)\n",
    "        for page in doc:\n",
    "            text = page.get_text()\n",
    "            chunks.append({\"text\": f\"Page {page.number}: {text}\", \n",
    "                          \"metadata\": {\"file_name\": file_name, \"file_type\": \"pdf\", \"page\": page.number}})\n",
    "        doc.close()\n",
    "    elif file_ext == '.txt':\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "            chunks.append({\"text\": text, \"metadata\": {\"file_name\": file_name, \"file_type\": \"txt\"}})\n",
    "    else:\n",
    "        return file_name, []\n",
    "    \n",
    "    # Split large texts\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "    split_chunks = []\n",
    "    for chunk in chunks:\n",
    "        split_texts = splitter.split_text(chunk[\"text\"])\n",
    "        for split_text in split_texts:\n",
    "            split_chunks.append({\"text\": split_text, \"metadata\": chunk[\"metadata\"]})\n",
    "    \n",
    "    return file_name, split_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 3: Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Command Extraction Module\n",
    "def extract_command(query: str) -> QueryCommand:\n",
    "    \"\"\"Use Mistral to parse query into a structured command.\"\"\"\n",
    "    prompt = (\n",
    "        \"You are a document repository assistant. Given this query: '{query}', identify the command \"\n",
    "        \"(e.g., 'summarize', 'answer_question', 'search', 'compare_files') and parameters \"\n",
    "        \"(e.g., file_name, keyword, file_type). Output JSON: {'command': '...', 'params': {'file_name': '...', 'keyword': '...', 'file_type': '...'}}. \"\n",
    "        \"Examples:\\n\"\n",
    "        \"Query: 'Summarize sales data' → {'command': 'summarize', 'params': {'keyword': 'sales', 'file_type': 'all'}}\\n\"\n",
    "        \"Query: 'Compare revenue in file1.xlsx and file2.xlsx' → {'command': 'compare_files', 'params': {'file_name': ['file1.xlsx', 'file2.xlsx'], 'keyword': 'revenue'}}\\n\"\n",
    "        \"Query: 'What is in report.pdf?' → {'command': 'answer_question', 'params': {'file_name': 'report.pdf', 'file_type': 'pdf'}}\"\n",
    "    )\n",
    "    response = ollama.chat(model='mistral:7b-instruct-v0.3-q4_0', messages=[\n",
    "        {'role': 'system', 'content': prompt},\n",
    "        {'role': 'user', 'content': f\"Query: {query}\"}\n",
    "    ])\n",
    "    try:\n",
    "        command_dict = json.loads(response['message']['content'])\n",
    "        return QueryCommand(**command_dict)\n",
    "    except:\n",
    "        return QueryCommand(command=\"answer_question\", params={\"keyword\": query, \"file_type\": \"all\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 4: Retrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Retrieval Module\n",
    "def setup_rag(chunks: List[Dict]) -> any:\n",
    "    \"\"\"Set up RAG pipeline with Chroma for repository-wide retrieval.\"\"\"\n",
    "    texts = [chunk[\"text\"] for chunk in chunks]\n",
    "    metadatas = [chunk[\"metadata\"] for chunk in chunks]\n",
    "    embeddings = OllamaEmbeddings(model='nomic-embed-text')\n",
    "    vectorstore = Chroma.from_texts(texts, embeddings, metadatas=metadatas, persist_directory='./db')\n",
    "    return vectorstore.as_retriever(search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 5: Response Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Response Generation Module\n",
    "def generate_response(chunks: List[Dict], query: str, command: QueryCommand) -> ChatbotResponse:\n",
    "    \"\"\"Generate a concise, document-grounded response using RAG.\"\"\"\n",
    "    retriever = setup_rag(chunks)\n",
    "    # Apply metadata filters if specified\n",
    "    filter_params = {}\n",
    "    if command.params.get(\"file_type\") != \"all\":\n",
    "        filter_params[\"file_type\"] = command.params[\"file_type\"]\n",
    "    if command.params.get(\"file_name\"):\n",
    "        filter_params[\"file_name\"] = {\"$in\": command.params[\"file_name\"]} if isinstance(command.params[\"file_name\"], list) else command.params[\"file_name\"]\n",
    "    retriever.search_kwargs[\"filter\"] = filter_params if filter_params else None\n",
    "    \n",
    "    llm = ChatOllama(model='mistral:7b-instruct-v0.3-q4_0')\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"Context: {context}\\nQuestion: {query}\\nProvide a concise answer (under 100 words) based only on the document data. Cite file names.\"\n",
    "    )\n",
    "    chain = {\"context\": retriever, \"query\": RunnablePassthrough()} | prompt | llm | StrOutputParser()\n",
    "    answer = chain.invoke(query)\n",
    "    \n",
    "    # Extract sources from retrieved documents\n",
    "    retrieved_docs = retriever.get_relevant_documents(query)\n",
    "    sources = list(set(doc.metadata.get(\"file_name\", \"unknown\") for doc in retrieved_docs))\n",
    "    \n",
    "    return ChatbotResponse(answer=answer, sources=sources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 6: Commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the Pipeline\n",
    "import glob\n",
    "\n",
    "# Load all documents in a directory (replace with your path)\n",
    "repo_path = \"./documents\"  # Create a folder and add .xlsx, .pdf, .txt files\n",
    "all_chunks = []\n",
    "for file_path in glob.glob(f\"{repo_path}/*\"):\n",
    "    file_name, chunks = load_document(file_path)\n",
    "    if chunks:\n",
    "        print(f\"Loaded {file_name}\")\n",
    "        all_chunks.extend(chunks)\n",
    "\n",
    "# Test query\n",
    "query = \"Summarize sales data\"\n",
    "response = handle_query(all_chunks, query)\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"Answer: {response.answer}\")\n",
    "print(f\"Sources: {response.sources}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 7: Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the Pipeline\n",
    "import glob\n",
    "\n",
    "# Load all documents in a directory (replace with your path)\n",
    "repo_path = \"./documents\"  # Create a folder and add .xlsx, .pdf, .txt files\n",
    "all_chunks = []\n",
    "for file_path in glob.glob(f\"{repo_path}/*\"):\n",
    "    file_name, chunks = load_document(file_path)\n",
    "    if chunks:\n",
    "        print(f\"Loaded {file_name}\")\n",
    "        all_chunks.extend(chunks)\n",
    "\n",
    "# Test query\n",
    "query = \"Summarize sales data\"\n",
    "response = handle_query(all_chunks, query)\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"Answer: {response.answer}\")\n",
    "print(f\"Sources: {response.sources}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streamlit UI (Save as separate app.py file and run with `streamlit run app.py`)\n",
    "import streamlit as st\n",
    "\n",
    "st.title(\"Local Document Repository RAG Chatbot\")\n",
    "uploaded_files = st.file_uploader(\"Upload documents\", type=['xlsx', 'xls', 'pdf', 'txt'], accept_multiple_files=True)\n",
    "if uploaded_files:\n",
    "    all_chunks = []\n",
    "    for file in uploaded_files:\n",
    "        file_path = f\"./temp/{file.name}\"\n",
    "        os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "        with open(file_path, \"wb\") as f:\n",
    "            f.write(file.read())\n",
    "        file_name, chunks = load_document(file_path)\n",
    "        if chunks:\n",
    "            all_chunks.extend(chunks)\n",
    "            st.success(f\"Loaded {file_name}\")\n",
    "    \n",
    "    query = st.text_input(\"Ask about the document repository (e.g., 'Summarize sales data', 'Compare revenue in file1.xlsx and file2.xlsx')\")\n",
    "    if query:\n",
    "        response = handle_query(all_chunks, query)\n",
    "        st.write(f\"**Answer**: {response.answer}\")\n",
    "        st.write(f\"**Sources**: {', '.join(response.sources)}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
